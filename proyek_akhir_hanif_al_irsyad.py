# -*- coding: utf-8 -*-
"""Proyek Akhir_Hanif Al Irsyad

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/100A857GKp6ZhvanmBA1S8u9C_K6w4P5u
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d muratkokludataset/rice-image-dataset

import zipfile

zip_ref = zipfile.ZipFile("/content/rice-image-dataset.zip", 'r')
zip_ref.extractall('/content')
zip_ref.close()

def list_files(path):
  files_num = 0
  for root, dirs, files in os.walk(path):
    level = root.replace(path, '').count(os.sep)
    indent = ' ' * 2 * (level)
    files_num += len(files)
    print('{}{}/ {}'.format(indent, os.path.basename(root), (str(len(files)) + ' images' if len(files) > 0 else '')))
  
  return files_num

import os
data_dir = '/content/Rice_Image_Dataset'
list_files(data_dir)

def read_files(path):
  image_files = []
  for dirname, dirnames, filenames in os.walk(path):
    for filename in filenames:
      image_files.append(os.path.join(dirname, filename))
  
  return image_files

import PIL

full_directory = read_files(data_dir)
image_sizes = []
for file in full_directory:
  image = PIL.Image.open(file)
  width, height = image.size
  image_sizes.append(f'{width}x{height}')

unique_sizes = set(image_sizes)

print(f'Image size list (first 15 unique size): \n{list(unique_sizes)[:15]}')

train_dir = os.path.join(data_dir, 'train')
validation_dir = os.path.join(data_dir, 'valid')

arborio_data = os.path.join(data_dir, 'arborio')
basmati_data = os.path.join(data_dir, 'basmati')
ipsala_data = os.path.join(data_dir, 'ipsala')
jasmine_data = os.path.join(data_dir, 'jasmine')
karacadag_data = os.path.join(data_dir, 'karacadag')

os.mkdir(train_dir)
os.mkdir(validation_dir)

arborio_train = os.path.join(train_dir, 'arborio')
basmati_train = os.path.join(train_dir, 'basmati')
ipsala_train = os.path.join(train_dir, 'ipsala')
jasmine_train = os.path.join(train_dir, 'jasmine')
karacadag_train = os.path.join(train_dir, 'karacadag')

arborio_valid = os.path.join(validation_dir, 'arborio')
basmati_valid = os.path.join(validation_dir, 'basmati')
ipsala_valid = os.path.join(validation_dir, 'ipsala')
jasmine_valid = os.path.join(validation_dir, 'jasmine')
karacadag_valid = os.path.join(validation_dir, 'karacadag')

os.mkdir(arborio_train)
os.mkdir(basmati_train)
os.mkdir(ipsala_train)
os.mkdir(jasmine_train)
os.mkdir(karacadag_train)
os.mkdir(arborio_valid)
os.mkdir(basmati_valid)
os.mkdir(ipsala_valid)
os.mkdir(jasmine_valid)
os.mkdir(karacadag_valid)

from sklearn.model_selection import train_test_split

latih_arborio_dir, val_arborio_dir = train_test_split(os.listdir(arborio_data), test_size=0.20)
latih_basmati_dir, val_basmati_dir = train_test_split(os.listdir(basmati_data), test_size=0.20)
latih_ipsala_dir, val_ipsala_dir = train_test_split(os.listdir(ipsala_data), test_size=0.20)
latih_jasmine_dir, val_jasmine_dir = train_test_split(os.listdir(jasmine_data), test_size=0.20)
latih_karacadag_dir, val_karacadag_dir = train_test_split(os.listdir(karacadag_data), test_size=0.20)

import shutil

for file in latih_arborio_dir:
  shutil.copy(os.path.join(arborio_data, file), os.path.join(arborio_train, file))
for file in latih_basmati_dir:
  shutil.copy(os.path.join(basmati_data, file), os.path.join(basmati_train, file))
for file in latih_ipsala_dir:
  shutil.copy(os.path.join(ipsala_data, file), os.path.join(ipsala_train, file))
for file in latih_jasmine_dir:
  shutil.copy(os.path.join(jasmine_data, file), os.path.join(jasmine_train, file))
for file in latih_karacadag_dir:
  shutil.copy(os.path.join(karacadag_data, file), os.path.join(karacadag_train, file))
for file in val_arborio_dir:
  shutil.copy(os.path.join(arborio_data, file), os.path.join(arborio_valid, file))
for file in val_basmati_dir:
  shutil.copy(os.path.join(basmati_data, file), os.path.join(basmati_valid, file))
for file in val_ipsala_dir:
  shutil.copy(os.path.join(ipsala_data, file), os.path.join(ipsala_valid, file))
for file in val_jasmine_dir:
  shutil.copy(os.path.join(jasmine_data, file), os.path.join(jasmine_valid, file))
for file in val_karacadag_dir:
  shutil.copy(os.path.join(karacadag_data, file), os.path.join(karacadag_valid, file))

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(validation_split=0.25,
                                   rescale=1/.255
                                  )

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(256, 256),
                                                    batch_size=32,
                                                    class_mode = "categorical")
valid_generator = train_datagen.flow_from_directory(validation_dir,
                                                    target_size=(256,256),
                                                    batch_size=32,
                                                    class_mode = "categorical")

class Callbackkuh(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.92) and (logs.get('val_accuracy')>=0.92):
            print("\nReached %2.2f%% accuracy, training has been stop" %(logs.get('accuracy')*100))
            self.model.stop_training = True

callbacks = Callbackkuh()

import tensorflow as tf
from tensorflow.python.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D

model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(5, activation=tf.nn.softmax)
])

model.compile(optimizer=tf.optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    train_generator,
    steps_per_epoch = 30,
    epochs = 50,
    validation_data = valid_generator,
    validation_steps = 20,
    verbose = 2,
    callbacks = [callbacks]
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Accuracy Model')
plt.legend(loc=0)
plt.figure()

plt.show()

plt.plot(epochs, loss, 'r', label='Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Loss Model')
plt.legend(loc=0)
plt.figure()

plt.show()

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite = converter.convert()

with tf.io.gfile.GFile('model_rice.tflite', 'wb') as f:
  f.write(tflite)